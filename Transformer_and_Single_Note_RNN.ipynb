{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer and Single-Note RNN",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thamsuppp/MusicGenDL/blob/main/Transformer_and_Single_Note_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9NUurTqwdWD"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg0AggLUwayE"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "import tables\n",
        "from music21 import *\n",
        "import music21\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "import json\n",
        "import IPython.display\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "from musicautobot.numpy_encode import *\n",
        "from musicautobot.config import *\n",
        "from musicautobot.music_transformer import *\n",
        "from musicautobot.multitask_transformer import *\n",
        "from musicautobot.utils import midifile\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "root_dir = 'drive/MyDrive/CIS522 Project'\n",
        "data_dir = root_dir + '/Lakh Piano Dataset/LPD-5/lpd_5/lpd_5_cleansed'\n",
        "music_dataset_lpd_dir = root_dir + '/Music Dataset/midis/lmd_matched'\n",
        "\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "!pip install music21\n",
        "!pip install pypianoroll\n",
        "!pip install musicautobot\n",
        "!pip install pebble\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd9iGNO0wjFC"
      },
      "source": [
        "**Getting MIDI and Song Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ-ZSVizwlH_"
      },
      "source": [
        "RESULTS_PATH = os.path.join(root_dir, 'Lakh Piano Dataset', 'Metadata')\n",
        "\n",
        "# Utility functions for retrieving paths\n",
        "def msd_id_to_dirs(msd_id):\n",
        "    \"\"\"Given an MSD ID, generate the path prefix.\n",
        "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
        "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
        "\n",
        "\n",
        "def msd_id_to_h5(msd_id):\n",
        "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
        "    return os.path.join(RESULTS_PATH, 'lmd_matched_h5',\n",
        "                        msd_id_to_dirs(msd_id) + '.h5')\n",
        "\n",
        "# Load the midi npz file from the LMD cleansed folder\n",
        "def get_midi_npz_path(msd_id, midi_md5):\n",
        "    return os.path.join(data_dir,\n",
        "                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')\n",
        "    \n",
        "# Load the midi file from the Music Dataset folder\n",
        "def get_midi_path(msd_id, midi_md5):\n",
        "    return os.path.join(music_dataset_lpd_dir,\n",
        "                        msd_id_to_dirs(msd_id), midi_md5 + '.mid')\n",
        "    \n",
        "# Open the cleansed ids - cleansed file ids : msd ids\n",
        "cleansed_ids = pd.read_csv(os.path.join(root_dir, 'Lakh Piano Dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n",
        "lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}\n",
        "msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cp7D4EdwqjA"
      },
      "source": [
        "**Reading Genre Annotations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhAGsLAvwz_W"
      },
      "source": [
        "genre_file_dir = os.path.join(root_dir, 'Lakh Piano Dataset', 'Genre', 'msd_tagtraum_cd1.cls')\n",
        "ids = []\n",
        "genres = []\n",
        "with open(genre_file_dir) as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        if line[0] != '#':\n",
        "          split = line.strip().split(\"\\t\")\n",
        "          if len(split) == 2:\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "          elif len(split) == 3:\n",
        "            ids.append(split[0])\n",
        "            ids.append(split[0])\n",
        "            genres.append(split[1])\n",
        "            genres.append(split[2])\n",
        "        line = f.readline()\n",
        "genre_df = pd.DataFrame(data={\"TrackID\": ids, \"Genre\": genres})\n",
        "\n",
        "genre_dict = genre_df.groupby('TrackID')['Genre'].apply(lambda x: x.tolist()).to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LRm5--9yBNx"
      },
      "source": [
        "**Retrieving MSD IDs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-1uS9DXyFgq"
      },
      "source": [
        "# Load the processed metadata\n",
        "with open(os.path.join(root_dir, 'Lakh Piano Dataset', 'processed_metadata.json'), 'r') as outfile:\n",
        "  lmd_metadata = json.load(outfile)\n",
        "\n",
        "# Change this into a dictionary of MSD_ID: metadata\n",
        "lmd_metadata = {e['msd_id']:e for e in lmd_metadata}\n",
        "\n",
        "# Get all song MSD IDs in pop rock genre\n",
        "pop_song_msd_ids = [k for k, v in lmd_metadata.items() if 'Pop_Rock' in v['genre_tagtraum']]\n",
        "\n",
        "# Filter by artist name\n",
        "carey_song_ids = [k for k, v in lmd_metadata.items() if 'Carey' in v['artist']]\n",
        "mj_song_ids = [k for k, v in lmd_metadata.items() if 'Michael Jackson' in v['artist']]\n",
        "green_song_ids = [k for k, v in lmd_metadata.items() if 'Green Day' in v['artist']]\n",
        "spice_song_ids = [k for k, v in lmd_metadata.items() if 'Spice Girls' in v['artist']]\n",
        "\n",
        "all = [k for k, v in lmd_metadata.items()]\n",
        "\n",
        "# Randomly choose 1000 songs out of these\n",
        "train_ids = random.choices(pop_song_msd_ids, k = 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIEWc2ypwVLF"
      },
      "source": [
        "**Transformer Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2nnBbRVv6pF"
      },
      "source": [
        "import pebble\n",
        "from musicautobot.numpy_encode import *\n",
        "from musicautobot.utils.file_processing import process_all, process_file\n",
        "from musicautobot.config import *\n",
        "from musicautobot.music_transformer import *\n",
        "\n",
        "\n",
        "# Location of your midi files\n",
        "midi_path = Path('drive/My Drive/CIS522 Project/spice midis')\n",
        "data_path = Path('data/numpy')\n",
        "data_save_name = 'musicitem_data_save.pkl'\n",
        "\n",
        "\n",
        "midi_files = get_files(midi_path, '.mid', recurse=True)\n",
        "data_mj = MusicDataBunch.from_files(midi_files, data_path, processors=[Midi2ItemProcessor()], bs=4, bptt=128, encode_position=False)\n",
        "\n",
        "learn_mj = music_model_learner(data, arch=TransformerXL, config=default_config())\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "learn_mj.fit_one_cycle(5)\n",
        "\n",
        "midi_file = Path('drive/MyDrive/CIS522 Project/spice midis/78152aeb43e3e22ccc608e330e5bcb92.mid')\n",
        "item = MusicItem.from_file(midi_file, data.vocab);\n",
        "pred = midi_file.predict(item, n_words=500)\n",
        "\n",
        "pred[0].to_stream().show('text')\n",
        "play(pred[0].to_stream())\n",
        "show(pred[0].to_stream())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIVxMPmUzLh2"
      },
      "source": [
        "**LSTM Single-Note Generation - Accesing Audio Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXdL-MbGzUwp"
      },
      "source": [
        "# Get all song MSD IDs satisfying the condition\n",
        "filtered_msd_ids = [k for k, v in lmd_metadata.items() if 'Michael Jackson' in v['artist']]\n",
        "\n",
        "# Randomly choose 1000 songs out of these\n",
        "train_ids = random.choices(filtered_msd_ids, k = 1000)\n",
        "\n",
        "# Loop that reads each song in train_ids, parses the PIANO notes and saves the string representation of the note in notes\n",
        "notes = []\n",
        "\n",
        "i = 0\n",
        "for msd_file_name in filtered_msd_ids:\n",
        "  lpd_file_name = msd_to_lpd_ids[msd_file_name]\n",
        "\n",
        "  # Get the NPZ path\n",
        "  npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)\n",
        "\n",
        "  multitrack = pypianoroll.load(npz_path)\n",
        "  pm = pypianoroll.to_pretty_midi(multitrack)\n",
        "  new_midi_path = npz_path[:-4] + '.mid'\n",
        "  pypianoroll.write(new_midi_path, multitrack)\n",
        "  # Get the MIDI path (should already be generated)\n",
        "  new_midi_path = npz_path[:-4] + '.mid'\n",
        "  midi = converter.parse(new_midi_path)\n",
        "\n",
        "  s2 = instrument.partitionByInstrument(midi)\n",
        "  piano_part = None\n",
        "  # Filter for  only the piano part\n",
        "  instr = instrument.Piano\n",
        "  for part in s2:\n",
        "    if isinstance(part.getInstrument(), instr):\n",
        "      piano_part = part\n",
        "\n",
        "  notes_song = []\n",
        "  if piano_part: # Some songs somehow have no piano parts\n",
        "    for element in piano_part:\n",
        "      if isinstance(element, note.Note):\n",
        "        # Return the pitch of the single note\n",
        "          notes_song.append(str(element.pitch))\n",
        "      elif isinstance(element, chord.Chord):\n",
        "        # Returns the normal order of a Chord represented in a list of integers\n",
        "          notes_song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "  notes.append(notes_song)\n",
        "  i+=1\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl93RLcOznjr"
      },
      "source": [
        "**Preparing Input and Output Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DSfYK97zwQ5"
      },
      "source": [
        "# Prepare input and output sequences\n",
        "\n",
        "def prepare_sequences(notes, note_to_int = None, sequence_length = 32):\n",
        "  network_input = []\n",
        "  network_output = []\n",
        "\n",
        "  if not note_to_int:\n",
        "    # Set of note/chords (collapse into list)\n",
        "    pitch_names = sorted(set(itertools.chain(*notes)))\n",
        "    # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitch_names))\n",
        "\n",
        "  # Loop through all songs\n",
        "  for song in notes:\n",
        "    # Check for the end\n",
        "    i = 0\n",
        "    while i + sequence_length < len(song):\n",
        "      # seq_len notes for the input seq\n",
        "      sequence_in = song[i: i + sequence_length]\n",
        "      # Next note to predict\n",
        "      sequence_out = song[i+sequence_length]\n",
        "      # Return the int representation of the note - *(If note not found)\n",
        "      network_input.append([note_to_int.get(char, 0) for char in sequence_in])\n",
        "      network_output.append(note_to_int.get(sequence_out, 0))\n",
        "      i += sequence_length\n",
        "\n",
        "  n_patterns = len(network_input)\n",
        "\n",
        "  # Reshape for LSTM input\n",
        "  network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "  # Normalize input (?? - CHECK LATER - this assumes the alphabetical order of the notes carries semantic meaning?)\n",
        "  #network_input = network_input / len(pitch_names)\n",
        "  #network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "  return network_input, network_output, note_to_int\n",
        "\n",
        "train_input, train_output, note_to_int = prepare_sequences(notes_train, sequence_length = 64)\n",
        "#test_input, test_output, _ = prepare_sequences(notes_test, note_to_int = note_to_int, sequence_length = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lhv-3eyz1rj"
      },
      "source": [
        "**RNN Generation Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbV6o-Qqz3Ga"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Take a random observation from the network input, return (input, target), each shifted by 1\n",
        "# NOT NEEDED ANYMORE - each epoch just using entire dataset\n",
        "def random_training_set(network_input):    \n",
        "    chunk = network_input[random.randint(0, network_input.shape[0] - 1), : , :]\n",
        "    input = torch.tensor(chunk[:-1], dtype = torch.long).squeeze()\n",
        "    target = torch.tensor(chunk[1:], dtype = torch.long).squeeze()\n",
        "    return input, target\n",
        "\n",
        "\n",
        "def grad_clipping(net, theta):  \n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    \n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm\n",
        "\n",
        "class GenerationRNN(nn.Module):\n",
        "  # input_size: number of possible pitches\n",
        "  # hidden_size: embedding size of each pitch\n",
        "  # output_size: number of possible pitches (probability distribution)\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(GenerationRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size * n_layers, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # Creates embedding of the input texts\n",
        "        #print('initial input', input.size())\n",
        "        input = self.embedding(input.view(1, -1))\n",
        "        #print('input after embedding', input.size())\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        #print('output after gru', output.size())\n",
        "        #print('hidden after gru', hidden.size())\n",
        "        output = self.decoder(hidden.view(1, -1))\n",
        "        #print('output after decoder', output.size())\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size).to(device)\n",
        "\n",
        "# Single training step for ONE sequence\n",
        "def train_sequence(input, target, model, optimizer, criterion):\n",
        "    # Initialize hidden state, zero the gradients of model \n",
        "    hidden = model.init_hidden()\n",
        "    model.zero_grad()\n",
        "    loss = 0\n",
        "    # For each character in our chunk (except last), compute the hidden and ouput\n",
        "    # Using each output, compute the loss with the corresponding target \n",
        "    for i in range(len(input)):\n",
        "        output, hidden = model(input[i], hidden)\n",
        "        loss += criterion(output, target[i].unsqueeze(0))\n",
        "    \n",
        "    # Backpropagate, clip gradient and optimize\n",
        "    loss.backward()\n",
        "    grad_clipping(model, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return loss.data.item() / len(input)\n",
        "\n",
        "def test_sequence(input, target, model, criterion):\n",
        "    # Initialize hidden state, zero the gradients of model \n",
        "    hidden = model.init_hidden()\n",
        "    model.zero_grad()\n",
        "    loss = 0\n",
        "    # For each character in our chunk (except last), compute the hidden and ouput\n",
        "    # Using each output, compute the loss with the corresponding target \n",
        "    for i in range(len(input)):\n",
        "        output, hidden = model(input[i], hidden)\n",
        "        loss += criterion(output, target[i].unsqueeze(0))\n",
        "\n",
        "    # Return average loss for the input sequence\n",
        "    return loss.data.item() / len(input)\n",
        "\n",
        "\n",
        "# Overall training loop\n",
        "def training_loop(model, optimizer, scheduler, criterion, train_input, test_input):\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    # Training - sample 2000\n",
        "    sampled_train_ids = random.choices(range(train_input.shape[0]), k = 2000)\n",
        "    print(scheduler.get_last_lr())\n",
        "    for i in range(train_input.shape[0]):\n",
        "      sequence = train_input[i, : , :]\n",
        "      input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)\n",
        "      target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)\n",
        "      loss = train_sequence(input, target, model, optimizer, criterion)\n",
        "      running_loss += loss\n",
        "\n",
        "    train_epoch_loss = running_loss / 2000\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    running_loss = 0\n",
        "    # model.eval()\n",
        "    # # Testing\n",
        "    # for i in range(test_input.shape[0]):\n",
        "    #   sequence = test_input[i, : , :]\n",
        "    #   input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)\n",
        "    #   target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)\n",
        "    #   loss = test_sequence(input, target, model, criterion)\n",
        "    #   running_loss += loss\n",
        "\n",
        "    # test_epoch_loss = running_loss / 1000\n",
        "    # test_losses.append(test_epoch_loss)\n",
        "    test_epoch_loss = 0\n",
        "\n",
        "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
        "\n",
        "  return train_losses, test_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Te-stYz_hz"
      },
      "source": [
        "n_pitches = len(note_to_int)\n",
        "hidden_size = 96\n",
        "n_layers = 2\n",
        "n_epochs = 40\n",
        "lr = 0.002\n",
        "lr_lambda = 0.99\n",
        "\n",
        "model = GenerationRNN(input_size = n_pitches, hidden_size = hidden_size, output_size = n_pitches, n_layers = n_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_input, train_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhVVyCXB0BpL"
      },
      "source": [
        "experiment_params_list = [\n",
        "                          {'HIDDEN': 128, 'N_LAYERS': 2, 'LR': 0.001, 'LR_LAMBDA': 1, 'OPTIM': 'Adam'},\n",
        "                          {'HIDDEN': 64, 'N_LAYERS': 4, 'LR': 0.001, 'LR_LAMBDA': 1, 'OPTIM': 'Adam'},\n",
        "                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.05, 'LR_LAMBDA': 0.95, 'OPTIM': 'SGD'},\n",
        "                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.025, 'LR_LAMBDA': 0.95, 'OPTIM': 'SGD'},\n",
        "                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.001, 'LR_LAMBDA': 0.975, 'OPTIM': 'Adam'},\n",
        "                          {'HIDDEN': 96, 'N_LAYERS': 2, 'LR': 0.0025, 'LR_LAMBDA': 0.95, 'OPTIM': 'Adam'},\n",
        "                    ]\n",
        "\n",
        "experiment_losses = {}\n",
        "experiment_num = 0\n",
        "\n",
        "for params in experiment_params_list:\n",
        "\n",
        "  n_pitches = len(note_to_int)\n",
        "  hidden_size = params['HIDDEN']\n",
        "  n_layers = params['N_LAYERS']\n",
        "  n_epochs = 50\n",
        "  lr = params['LR']\n",
        "  lr_lambda = params['LR_LAMBDA']\n",
        "\n",
        "  print(experiment_num, params)\n",
        "\n",
        "  # Create model, optimizer and loss function\n",
        "  model = GenerationRNN(input_size = n_pitches, hidden_size = hidden_size, output_size = n_pitches, n_layers = n_layers).to(device)\n",
        "  if params['OPTIM'] == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "  else:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_input, test_input)\n",
        "\n",
        "  # Save Model\n",
        "  model_name = 'RNN 21 Apr Overnight Old exp{}'.format(experiment_num)\n",
        "  save_path = os.path.join(root_dir, 'Saved Models', model_name)\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "\n",
        "  # Save experiment losses\n",
        "  experiment_losses.update({experiment_num: {'train_losses': train_losses, 'test_losses': test_losses}})\n",
        "\n",
        "  # Plot the losses over epochs\n",
        "  plt.figure()\n",
        "  plt.plot(train_losses, label = 'Train Loss')\n",
        "  plt.plot(test_losses, label = 'Test Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.title(experiment_num)\n",
        "  plt.show()\n",
        "  try:\n",
        "    with open(os.path.join(root_dir, 'experiment_losses 20 Apr.json'), 'w') as outfile:\n",
        "      json.dump(experiment_losses, outfile)\n",
        "  except:\n",
        "    print('failed to save')\n",
        "\n",
        "  experiment_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioqzU3a70DLP"
      },
      "source": [
        "# Plot the losses over epochs\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label = 'Train Loss')\n",
        "plt.plot(test_losses, label = 'Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title(experiment_num)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGG7Ja3M0Ik5"
      },
      "source": [
        "**RNN Single-Note Generation Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjHQTgj_0Fb7"
      },
      "source": [
        "# Code to evaluate the language model i.e. generate new music\n",
        "\n",
        "def evaluate(net, prime_seq, predict_len):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = net.init_hidden()\n",
        "\n",
        "    predicted = prime_seq.copy()\n",
        "    prime_seq = torch.tensor(prime_seq, dtype = torch.long).to(device)\n",
        "\n",
        "\n",
        "    # \"Building up\" the hidden state using the prime sequence\n",
        "    for p in range(len(prime_seq) - 1):\n",
        "        input = prime_seq[p]\n",
        "        _, hidden = net(input, hidden)\n",
        "    \n",
        "    # Last character of prime sequence\n",
        "    input = prime_seq[-1]\n",
        "    \n",
        "    # For every index to predict\n",
        "    for p in range(predict_len):\n",
        "\n",
        "        # Pass the inputs to the model - output has dimension n_pitches - scores for each of the possible characters\n",
        "        output, hidden = net(input, hidden)\n",
        "\n",
        "        # Pick the character with the highest probability \n",
        "        predicted_id = torch.argmax(torch.softmax(output, dim = 1))\n",
        "\n",
        "        # Add predicted index to the list and use as next input\n",
        "        predicted.append(predicted_id.item()) \n",
        "        input = predicted_id\n",
        "\n",
        "    return predicted\n",
        "\n",
        "def evaluateMultinomial(net, prime_seq, predict_len, temperature=0.8):\n",
        "    '''\n",
        "    Arguments:\n",
        "    prime_seq - priming sequence (converted t)\n",
        "    predict_len - number of notes to predict for after prime sequence\n",
        "    '''\n",
        "    hidden = net.init_hidden()\n",
        "\n",
        "    predicted = prime_seq.copy()\n",
        "    prime_seq = torch.tensor(prime_seq, dtype = torch.long).to(device)\n",
        "\n",
        "\n",
        "    # \"Building up\" the hidden state using the prime sequence\n",
        "    for p in range(len(prime_seq) - 1):\n",
        "        input = prime_seq[p]\n",
        "        _, hidden = net(input, hidden)\n",
        "    \n",
        "    # Last character of prime sequence\n",
        "    input = prime_seq[-1]\n",
        "    \n",
        "    # For every index to predict\n",
        "    for p in range(predict_len):\n",
        "\n",
        "        # Pass the inputs to the model - output has dimension n_pitches - scores for each of the possible characters\n",
        "        output, hidden = net(input, hidden)\n",
        "        # Sample from the network output as a multinomial distribution\n",
        "        output = output.data.view(-1).div(temperature).exp()\n",
        "        predicted_id = torch.multinomial(output, 1)\n",
        "\n",
        "        # Add predicted index to the list and use as next input\n",
        "        predicted.append(predicted_id.item()) \n",
        "        input = predicted_id\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLdo4UiQ0OI-"
      },
      "source": [
        "generated_seq = evaluate(model, [100, 101, 102, 101, 100], predict_len = 100)\n",
        "generated_seq_multinomial = evaluateMultinomial(model, [100, 101, 102, 101, 100], predict_len = 500, temperature = 1.2)\n",
        "print(generated_seq)\n",
        "print(generated_seq_multinomial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icnrig_R0QsS"
      },
      "source": [
        "**Converting Generated Sequences Into MIDI/Audio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g4FIduX0Pt7"
      },
      "source": [
        "# Convert the generated ints into notes\n",
        "generated_seq = [int_to_note[e] for e in generated_seq]\n",
        "generated_seq_multinomial = [int_to_note[e] for e in generated_seq_multinomial]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVFg1Ql80UpY"
      },
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    return midi_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqnLRBT0XUz"
      },
      "source": [
        "# Load the generated MIDI\n",
        "generated_multitrack = pypianoroll.read(generated_path)\n",
        "generated_pm = pypianoroll.to_pretty_midi(generated_multitrack)\n",
        "generated_midi_audio = generated_pm.fluidsynth()\n",
        "IPython.display.Audio(generated_midi_audio, rate = 44100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}